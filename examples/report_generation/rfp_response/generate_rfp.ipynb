{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Umesh-Bhati/rfp-datasets/blob/main/examples/report_generation/rfp_response/generate_rfp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8e8f1c8-44e8-461d-9a12-13e714448fa1",
      "metadata": {
        "id": "f8e8f1c8-44e8-461d-9a12-13e714448fa1"
      },
      "source": [
        "# RFP Response Generation Workflow\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_parse/blob/main/examples/report_generation/rfp_response/generate_rfp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook shows you how to build a workflow to generate a response to an RFP.\n",
        "\n",
        "In this scenario, we assume that you are Microsoft, and you are responding to the [JEDI Cloud RFP](https://imlive.s3.amazonaws.com/Federal%20Government/ID151830346965529215587195222610265670631/HQ0034-18-R-0077.pdf) put out by the federal government. The government is using the submitted responses to decide the best vendor for their needs.\n",
        "\n",
        "![generate_rfp_img](https://github.com/run-llama/llama_parse/blob/main/examples/report_generation/rfp_response/generate_rfp_img.png?raw=1)\n",
        "\n",
        "We index a set of relevant documents that Microsoft has - including its annual report, wikipedia page on Microsoft Azure, a slide deck on the government cloud and cybersecurity capabilities. We then help you build an agentic workflow that can ingest an RFP, and generate a response for it in\n",
        "a way that adheres to its guidelines.\n",
        "\n",
        "We use LlamaParse to parse the context documents as well as the RFP document itself.\n",
        "\n",
        "**NOTE**: If you want to skip the indexing complexity and use LlamaCloud instead, check out the [RFP Example using LlamaCloud](https://github.com/run-llama/llamacloud-demo/blob/main/examples/report_generation/rfp_response/generate_rfp.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index  llama-parse llama-index-llms-groq"
      ],
      "metadata": {
        "id": "86fk9EELKGdV"
      },
      "id": "86fk9EELKGdV",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c4f28c6a-cb5e-4c16-bdc7-a69817fc4c12",
      "metadata": {
        "id": "c4f28c6a-cb5e-4c16-bdc7-a69817fc4c12"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7137b3-5b37-4e52-bc46-b994693d2664",
      "metadata": {
        "id": "cc7137b3-5b37-4e52-bc46-b994693d2664"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We download the [JEDI RFP template](https://imlive.s3.amazonaws.com/Federal%20Government/ID151830346965529215587195222610265670631/HQ0034-18-R-0077.pdf).\n",
        "\n",
        "We also download the context documents for Microsoft:\n",
        "1. Microsoft 2024 10-K\n",
        "2. Azure Wikipedia page\n",
        "3. A slide deck on Microsoft Azure Government\n",
        "4. Microsoft Digital Defense Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f1310efa-1422-4214-b0bf-41b6e163fac3",
      "metadata": {
        "id": "f1310efa-1422-4214-b0bf-41b6e163fac3",
        "outputId": "ddfcce3b-9830-495e-f3a0-f19c4f5d53df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-29 07:00:12--  https://imlive.s3.amazonaws.com/Federal%20Government/ID151830346965529215587195222610265670631/HQ0034-18-R-0077.pdf\n",
            "Resolving imlive.s3.amazonaws.com (imlive.s3.amazonaws.com)... 52.219.193.25, 52.219.116.90, 52.219.220.185, ...\n",
            "Connecting to imlive.s3.amazonaws.com (imlive.s3.amazonaws.com)|52.219.193.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 864798 (845K) [application/pdf]\n",
            "Saving to: ‘data/jedi_cloud_rfp.pdf’\n",
            "\n",
            "data/jedi_cloud_rfp 100%[===================>] 844.53K  2.34MB/s    in 0.4s    \n",
            "\n",
            "2024-12-29 07:00:13 (2.34 MB/s) - ‘data/jedi_cloud_rfp.pdf’ saved [864798/864798]\n",
            "\n",
            "--2024-12-29 07:00:13--  https://www.dropbox.com/scl/fi/4v5dx8dc9yqc8k0yw5g4h/msft_10k_2024.pdf?rlkey=jdyfrsoyb18ztlq5msunmibns&st=9w6bdyvn&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucca728bff05d4b600f7705887da.dl.dropboxusercontent.com/cd/0/inline/ChITIHiUcKFCaWAgPWu_PPw83WkiP7fi1j6g6feb6IRDnkiANQljs29kyMa2oDHvyU5wNk5oIMwB_JfUR-3xmyY0CgROvRU8ZDBAn-euT0Gw-FqcoxO6XZ1b-hxfOYvc1m-2gFrWK8RFv5dISdxTkd88/file?dl=1# [following]\n",
            "--2024-12-29 07:00:14--  https://ucca728bff05d4b600f7705887da.dl.dropboxusercontent.com/cd/0/inline/ChITIHiUcKFCaWAgPWu_PPw83WkiP7fi1j6g6feb6IRDnkiANQljs29kyMa2oDHvyU5wNk5oIMwB_JfUR-3xmyY0CgROvRU8ZDBAn-euT0Gw-FqcoxO6XZ1b-hxfOYvc1m-2gFrWK8RFv5dISdxTkd88/file?dl=1\n",
            "Resolving ucca728bff05d4b600f7705887da.dl.dropboxusercontent.com (ucca728bff05d4b600f7705887da.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucca728bff05d4b600f7705887da.dl.dropboxusercontent.com (ucca728bff05d4b600f7705887da.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/ChKwgUSWE8mQGj1QGQLxaIdDMzf5v7QCa42hcz7ONDq0nOlJszAgOI50Z3zZ7bb6CK8JRVASrJ1yiDHIPkjaz2mVDMfYHCK8up9YSZejtbFUoF9jBVwbgUC-NUVciGJrOrgwxhAfrUVnZ3AzF0dfv-03NRhso0XVJRCIMWvpKFlzQawax0Qd6nW1z5tW6-pkHFE5bP7EQVNTe9aLtJpnzCHqb0dgU5DwzaQJhw9Pts7ovhmumfObbaNJUXAFOGufVYS3_Hlh2tQOSQycx6vo6DFFnC5V_PEc_OlWv6Rion-nZBZNY1HeStrLw21Q1HY2qiXdEwOgp2MNSruHDvaFwJBShw2xGatT8rxEVkhtqb3blXMEPTT-q-4Gyz96CBF44CE/file?dl=1 [following]\n",
            "--2024-12-29 07:00:14--  https://ucca728bff05d4b600f7705887da.dl.dropboxusercontent.com/cd/0/inline2/ChKwgUSWE8mQGj1QGQLxaIdDMzf5v7QCa42hcz7ONDq0nOlJszAgOI50Z3zZ7bb6CK8JRVASrJ1yiDHIPkjaz2mVDMfYHCK8up9YSZejtbFUoF9jBVwbgUC-NUVciGJrOrgwxhAfrUVnZ3AzF0dfv-03NRhso0XVJRCIMWvpKFlzQawax0Qd6nW1z5tW6-pkHFE5bP7EQVNTe9aLtJpnzCHqb0dgU5DwzaQJhw9Pts7ovhmumfObbaNJUXAFOGufVYS3_Hlh2tQOSQycx6vo6DFFnC5V_PEc_OlWv6Rion-nZBZNY1HeStrLw21Q1HY2qiXdEwOgp2MNSruHDvaFwJBShw2xGatT8rxEVkhtqb3blXMEPTT-q-4Gyz96CBF44CE/file?dl=1\n",
            "Reusing existing connection to ucca728bff05d4b600f7705887da.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13360815 (13M) [application/binary]\n",
            "Saving to: ‘data/msft_10k_2024.pdf’\n",
            "\n",
            "data/msft_10k_2024. 100%[===================>]  12.74M  48.9MB/s    in 0.3s    \n",
            "\n",
            "2024-12-29 07:00:15 (48.9 MB/s) - ‘data/msft_10k_2024.pdf’ saved [13360815/13360815]\n",
            "\n",
            "--2024-12-29 07:00:15--  https://www.dropbox.com/scl/fi/7waur8ravmve3fe8nej0k/azure_wiki.pdf?rlkey=icru2w64oylx1p76ftt6y9irv&st=fr87vxob&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc8ac61b86e6353663e920f68b8e.dl.dropboxusercontent.com/cd/0/inline/ChK9CJvJ5VvRkInVl6_pvj1zga0gMya-7wCwDLVFSkkVmVqPAA6zxLsYbjCZazR31C68nvauPXIsXv7yESG8pNoCKg3eN6PnbjEGue1b69zXt_1GtPT4naWLYqfs877Ld_AcORfAjMApxWULyU7Id3iw/file?dl=1# [following]\n",
            "--2024-12-29 07:00:16--  https://uc8ac61b86e6353663e920f68b8e.dl.dropboxusercontent.com/cd/0/inline/ChK9CJvJ5VvRkInVl6_pvj1zga0gMya-7wCwDLVFSkkVmVqPAA6zxLsYbjCZazR31C68nvauPXIsXv7yESG8pNoCKg3eN6PnbjEGue1b69zXt_1GtPT4naWLYqfs877Ld_AcORfAjMApxWULyU7Id3iw/file?dl=1\n",
            "Resolving uc8ac61b86e6353663e920f68b8e.dl.dropboxusercontent.com (uc8ac61b86e6353663e920f68b8e.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc8ac61b86e6353663e920f68b8e.dl.dropboxusercontent.com (uc8ac61b86e6353663e920f68b8e.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/ChL_Gyi2vQ34EEXAFjcElen0EhXO2utvsRpueD-sV8RWiMMNd2k9lNs1Wq31BVaPReW-JfgO1uCaPsRbStrBNbpU-ri7c8KAfIAOgeyxXTz9WXhvz3nT7zUECcfUcrk5tLbEBWUsVHHBG-Ep1MaKxIgDGcNx4nwsia4XYVOfTndGCDGAUdo7fVT-ufmrvhfK-Ih7jwVJq0LiPxRbcUxcCFcaor6CIaXFBKQb7cKRJszDl3y6czntsP1tq6y1QOtmSSnSI5IiaihWUItCVs9ho3gCKv1vK9vXGFo6mtn6j5xs_1Nv19MD9c-X5HeYcnsVcIK1v93MPRTy3ZfX4kCh7L_J0l1y0svdP3A00yt6NcY0IJC6lDM0pmYEgCziQJ94pFM/file?dl=1 [following]\n",
            "--2024-12-29 07:00:17--  https://uc8ac61b86e6353663e920f68b8e.dl.dropboxusercontent.com/cd/0/inline2/ChL_Gyi2vQ34EEXAFjcElen0EhXO2utvsRpueD-sV8RWiMMNd2k9lNs1Wq31BVaPReW-JfgO1uCaPsRbStrBNbpU-ri7c8KAfIAOgeyxXTz9WXhvz3nT7zUECcfUcrk5tLbEBWUsVHHBG-Ep1MaKxIgDGcNx4nwsia4XYVOfTndGCDGAUdo7fVT-ufmrvhfK-Ih7jwVJq0LiPxRbcUxcCFcaor6CIaXFBKQb7cKRJszDl3y6czntsP1tq6y1QOtmSSnSI5IiaihWUItCVs9ho3gCKv1vK9vXGFo6mtn6j5xs_1Nv19MD9c-X5HeYcnsVcIK1v93MPRTy3ZfX4kCh7L_J0l1y0svdP3A00yt6NcY0IJC6lDM0pmYEgCziQJ94pFM/file?dl=1\n",
            "Reusing existing connection to uc8ac61b86e6353663e920f68b8e.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1286244 (1.2M) [application/binary]\n",
            "Saving to: ‘data/azure_wiki.pdf’\n",
            "\n",
            "data/azure_wiki.pdf 100%[===================>]   1.23M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-12-29 07:00:17 (11.1 MB/s) - ‘data/azure_wiki.pdf’ saved [1286244/1286244]\n",
            "\n",
            "--2024-12-29 07:00:17--  https://cdn.ymaws.com/flclerks.site-ym.com/resource/resmgr/2017_Fall_Conf/Presentations/2018-10-12_FCCC_Microsoft_Az.pdf\n",
            "Resolving cdn.ymaws.com (cdn.ymaws.com)... 3.161.163.12, 3.161.163.78, 3.161.163.101, ...\n",
            "Connecting to cdn.ymaws.com (cdn.ymaws.com)|3.161.163.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4265829 (4.1M) [application/pdf]\n",
            "Saving to: ‘data/azure_gov.pdf’\n",
            "\n",
            "data/azure_gov.pdf  100%[===================>]   4.07M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-12-29 07:00:17 (31.1 MB/s) - ‘data/azure_gov.pdf’ saved [4265829/4265829]\n",
            "\n",
            "--2024-12-29 07:00:17--  https://www.dropbox.com/scl/fi/qh00xz29rlom4md8ce675/microsoft_ddr.pdf?rlkey=d868nbnsu1ng41y1chw69y64b&st=24iqemb1&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdf4ed0e95554752df4d6bfd784.dl.dropboxusercontent.com/cd/0/inline/ChIID8XoGpQmIb20ktIYoTIszOfoKgKcuWM7w5CzJIw_rtDCKmzZ-u_x3jvzTGjsRbRY9SWr0BLkaA8If50JaHRZxF5_mSWrbUMrT83dUUQJOGE850kaJQuP09MAhJCRstLTPnaU7qFma_AL-TIP8_L1/file?dl=1# [following]\n",
            "--2024-12-29 07:00:18--  https://ucdf4ed0e95554752df4d6bfd784.dl.dropboxusercontent.com/cd/0/inline/ChIID8XoGpQmIb20ktIYoTIszOfoKgKcuWM7w5CzJIw_rtDCKmzZ-u_x3jvzTGjsRbRY9SWr0BLkaA8If50JaHRZxF5_mSWrbUMrT83dUUQJOGE850kaJQuP09MAhJCRstLTPnaU7qFma_AL-TIP8_L1/file?dl=1\n",
            "Resolving ucdf4ed0e95554752df4d6bfd784.dl.dropboxusercontent.com (ucdf4ed0e95554752df4d6bfd784.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucdf4ed0e95554752df4d6bfd784.dl.dropboxusercontent.com (ucdf4ed0e95554752df4d6bfd784.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/ChI4C1DvfViuZQOMCKb-FqZSj5o1Q4_VYanY9h92OwtrC7xmFqyo7RFzGnEMO3sMzzucdZDbGNEPqfo5Kn6pXusYzLCJK7-AkFJJwN-gaTh-QX41ZHmw0X-Fs1lCCN2s5gzCtv-I4XYqXY8cmSC5MLf_IFuYCVvk85GLfKJnn1BMtVhr0Cz0_kLZ1H1clcGwdsBdAiwJuE1-3PAkQwMemvm6PWhJSYzAytC45MJuJ_y-IKPdsf5ihWJbBtjWsUSK6I2KM1vec43uvu95hRByZ0bo0cJGAp585BmE6l3EOyeRz44cXgbC0nKtUkSwBK6JYHfyK66SCOpC_0rMUNG3Vmx9Bu1rGXxmnlM1zRYqrTBHd4rhGUleMsxceJOKeKdCA0s/file?dl=1 [following]\n",
            "--2024-12-29 07:00:19--  https://ucdf4ed0e95554752df4d6bfd784.dl.dropboxusercontent.com/cd/0/inline2/ChI4C1DvfViuZQOMCKb-FqZSj5o1Q4_VYanY9h92OwtrC7xmFqyo7RFzGnEMO3sMzzucdZDbGNEPqfo5Kn6pXusYzLCJK7-AkFJJwN-gaTh-QX41ZHmw0X-Fs1lCCN2s5gzCtv-I4XYqXY8cmSC5MLf_IFuYCVvk85GLfKJnn1BMtVhr0Cz0_kLZ1H1clcGwdsBdAiwJuE1-3PAkQwMemvm6PWhJSYzAytC45MJuJ_y-IKPdsf5ihWJbBtjWsUSK6I2KM1vec43uvu95hRByZ0bo0cJGAp585BmE6l3EOyeRz44cXgbC0nKtUkSwBK6JYHfyK66SCOpC_0rMUNG3Vmx9Bu1rGXxmnlM1zRYqrTBHd4rhGUleMsxceJOKeKdCA0s/file?dl=1\n",
            "Reusing existing connection to ucdf4ed0e95554752df4d6bfd784.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20147265 (19M) [application/binary]\n",
            "Saving to: ‘data/msft_ddr.pdf’\n",
            "\n",
            "data/msft_ddr.pdf   100%[===================>]  19.21M  54.8MB/s    in 0.4s    \n",
            "\n",
            "2024-12-29 07:00:20 (54.8 MB/s) - ‘data/msft_ddr.pdf’ saved [20147265/20147265]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download JEDI Cloud RFP Template\n",
        "!wget \"https://imlive.s3.amazonaws.com/Federal%20Government/ID151830346965529215587195222610265670631/HQ0034-18-R-0077.pdf\" -O data/jedi_cloud_rfp.pdf\n",
        "# microsoft annual report\n",
        "!wget \"https://www.dropbox.com/scl/fi/4v5dx8dc9yqc8k0yw5g4h/msft_10k_2024.pdf?rlkey=jdyfrsoyb18ztlq5msunmibns&st=9w6bdyvn&dl=1\" -O data/msft_10k_2024.pdf\n",
        "# !wget \"https://microsoft.gcs-web.com/static-files/1c864583-06f7-40cc-a94d-d11400c83cc8\" -O data/msft_10k_2024.pdf\n",
        "\n",
        "# azure wikipedia page\n",
        "!wget \"https://www.dropbox.com/scl/fi/7waur8ravmve3fe8nej0k/azure_wiki.pdf?rlkey=icru2w64oylx1p76ftt6y9irv&st=fr87vxob&dl=1\" -O data/azure_wiki.pdf\n",
        "# azure government slide deck\n",
        "!wget \"https://cdn.ymaws.com/flclerks.site-ym.com/resource/resmgr/2017_Fall_Conf/Presentations/2018-10-12_FCCC_Microsoft_Az.pdf\" -O data/azure_gov.pdf\n",
        "# microsoft cybersecurity capabilities\n",
        "!wget \"https://www.dropbox.com/scl/fi/qh00xz29rlom4md8ce675/microsoft_ddr.pdf?rlkey=d868nbnsu1ng41y1chw69y64b&st=24iqemb1&dl=1\" -O data/msft_ddr.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d28f1ce-9618-47a2-9fc9-0502a382d841",
      "metadata": {
        "id": "3d28f1ce-9618-47a2-9fc9-0502a382d841"
      },
      "source": [
        "We then parse the context documents with LlamaParse - we use multimodal mode in order to extract text from visual data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c5cddd64-b886-4b0a-b010-2c0168b42147",
      "metadata": {
        "id": "c5cddd64-b886-4b0a-b010-2c0168b42147"
      },
      "outputs": [],
      "source": [
        "from llama_parse import LlamaParse\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = userdata.get(\"LLAMA_CLOUD_API_KEY\")\n",
        "# use our multimodal models for extractions\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    use_vendor_multimodal_model=True,\n",
        "    vendor_multimodal_model_name=\"anthropic-sonnet-3.5\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2f85d79c-8835-4684-9883-f6342ceb44b0",
      "metadata": {
        "id": "2f85d79c-8835-4684-9883-f6342ceb44b0"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "data_dir = \"data\"\n",
        "data_out_dir = \"data_out_rfp\"\n",
        "\n",
        "!mkdir -p {data_dir}\n",
        "!mkdir -p {data_out_dir}\n",
        "\n",
        "# note: we skip the rfp doc, which will be indexed as part of the workflow\n",
        "files = [\"azure_gov.pdf\", \"azure_wiki.pdf\", \"msft_10k_2024.pdf\", \"msft_ddr.pdf\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c24920ca-878e-4aae-89e2-8fe36b0fb06f",
      "metadata": {
        "id": "c24920ca-878e-4aae-89e2-8fe36b0fb06f"
      },
      "source": [
        "**Option 1**: If you haven't parsed the files yet, run the block below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c58cc182-663a-4b30-9a70-5af22acbe83d",
      "metadata": {
        "id": "c58cc182-663a-4b30-9a70-5af22acbe83d",
        "outputId": "acbf9a46-84bb-4102-c93e-505f8d646d70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 1e907583-8832-491b-91e2-dba962065644\n",
            ".Started parsing the file under job_id 67b5950b-ad94-44f8-90d0-55e1065fe50a\n",
            ".Error while parsing the file 'data/msft_10k_2024.pdf': Failed to parse the file: {\"detail\":\"You've exceeded the maximum number of pages you can parse in a day (1000). Please contact support to increase your limit.\"}\n",
            "Error while parsing the file 'data/msft_ddr.pdf': Failed to parse the file: {\"detail\":\"You've exceeded the maximum number of pages you can parse in a day (1000). Please contact support to increase your limit.\"}\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "file_dicts = {}\n",
        "\n",
        "for f in files:\n",
        "    file_base = Path(f).stem\n",
        "    full_file_path = str(Path(data_dir) / f)\n",
        "\n",
        "    file_docs = parser.load_data(full_file_path)\n",
        "\n",
        "    # attach metadata\n",
        "    for idx, d in enumerate(file_docs):\n",
        "        d.metadata[\"file_path\"] = f\n",
        "        d.metadata[\"page_num\"] = idx + 1\n",
        "\n",
        "    file_dicts[f] = {\"file_path\": full_file_path, \"docs\": file_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff7e8ed-8c2a-4434-a7f9-e695dabc6158",
      "metadata": {
        "id": "dff7e8ed-8c2a-4434-a7f9-e695dabc6158"
      },
      "source": [
        "#### Generate Summaries for each file\n",
        "\n",
        "Now that we've loaded the chunks for each file, let's now generate a summary for each file. These summaries will then be attached to the tool descriptions to help inform the agent on which files to query to fetch the right information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "138f8dda-4e55-4917-b277-63645b3b4e0e",
      "metadata": {
        "id": "138f8dda-4e55-4917-b277-63645b3b4e0e",
        "outputId": "e7438531-2498-4c96-f8a0-80c1092c048d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Generate summary for file azure_gov.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
            "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Generated summary: This file, \"azure_gov.pdf\", is a marketing brochure for Microsoft Azure Government, showcasing its features, security, and compliance with government regulations, as well as its adoption by US government agencies.\n",
            ">> Generate summary for file azure_wiki.pdf\n",
            ">> Generated summary: This file is a comprehensive collection of news articles and updates about Microsoft Azure, including instances of service outages, performance issues, and other notable events related to the cloud platform.\n",
            ">> Generate summary for file msft_10k_2024.pdf\n",
            ">> Generated summary: Empty Response\n",
            ">> Generate summary for file msft_ddr.pdf\n",
            ">> Generated summary: Empty Response\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "summary_llm = Groq(\"llama3-70b-8192\")\n",
        "\n",
        "for f in files:\n",
        "    print(f\">> Generate summary for file {f}\")\n",
        "    index = SummaryIndex(file_dicts[f][\"docs\"])\n",
        "    response = index.as_query_engine(llm=summary_llm).query(\n",
        "        \"Generate a short 1-2 line summary of this file to help inform an agent on what this file is about.\"\n",
        "    )\n",
        "    print(f\">> Generated summary: {str(response)}\")\n",
        "    file_dicts[f][\"summary\"] = str(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bqlQCOVQdEGH"
      },
      "id": "bqlQCOVQdEGH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "430fd045-f1a4-47c2-a773-718608d4c059",
      "metadata": {
        "id": "430fd045-f1a4-47c2-a773-718608d4c059"
      },
      "outputs": [],
      "source": [
        "pickle.dump(file_dicts, open(f\"{data_out_dir}/tmp_file_dicts.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93decb8b-acb6-42fb-b7ef-2a2b0d264da3",
      "metadata": {
        "id": "93decb8b-acb6-42fb-b7ef-2a2b0d264da3"
      },
      "source": [
        "**Option 2**: If you've already run parsing, the files should be cached. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a6849b-4572-4315-b7d5-77ad3b7ec351",
      "metadata": {
        "id": "68a6849b-4572-4315-b7d5-77ad3b7ec351"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "file_dicts = pickle.load(open(f\"{data_out_dir}/tmp_file_dicts.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d5000db-d5ca-4522-8dfa-bc64567b1884",
      "metadata": {
        "id": "4d5000db-d5ca-4522-8dfa-bc64567b1884"
      },
      "source": [
        "### Build Indexes\n",
        "\n",
        "Once the text nodes are ready, we feed into our vector store, which will index these nodes into Chroma (you're welcome to use our other 40+ vector store integrations if you'd like)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a31409f-815d-4569-8b74-60fcec5af211",
      "metadata": {
        "id": "9a31409f-815d-4569-8b74-60fcec5af211"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index-vector-stores-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "101920ab-4054-4164-9f29-c3c9025c13bb",
      "metadata": {
        "id": "101920ab-4054-4164-9f29-c3c9025c13bb"
      },
      "outputs": [],
      "source": [
        "# Run if you want to recreate the index\n",
        "!rm -rf storage_rfp_chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b1a2cb3-d21b-4ccf-81a0-1a83d0514a5d",
      "metadata": {
        "id": "6b1a2cb3-d21b-4ccf-81a0-1a83d0514a5d"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "persist_dir = \"storage_rfp_chroma\"\n",
        "\n",
        "vector_store = ChromaVectorStore.from_params(\n",
        "    collection_name=\"rfp_docs\", persist_dir=persist_dir\n",
        ")\n",
        "index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b38f44aa-8dfa-4267-9489-d876ce056931",
      "metadata": {
        "id": "b38f44aa-8dfa-4267-9489-d876ce056931"
      },
      "source": [
        "**NOTE**: Don't run the block below if you've already inserted the nodes. Only run if it's your first time!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72fcbaf7-9552-4ddb-9d7f-c499126c8735",
      "metadata": {
        "id": "72fcbaf7-9552-4ddb-9d7f-c499126c8735"
      },
      "outputs": [],
      "source": [
        "all_nodes = [c for d in file_dicts.values() for c in d[\"docs\"]]\n",
        "index.insert_nodes(all_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0c4446-c477-4261-9747-c5294b0a54e9",
      "metadata": {
        "id": "bf0c4446-c477-4261-9747-c5294b0a54e9"
      },
      "source": [
        "### Define Retrievers\n",
        "\n",
        "Define retrievers, one for each file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bbac9ab-e3e2-448f-9e74-ad995415fc27",
      "metadata": {
        "id": "0bbac9ab-e3e2-448f-9e74-ad995415fc27"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.vector_stores import (\n",
        "    MetadataFilter,\n",
        "    MetadataFilters,\n",
        "    FilterOperator,\n",
        ")\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from typing import List\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "# function tools\n",
        "def generate_tool(file: str, file_description: Optional[str] = None):\n",
        "    \"\"\"Return a function that retrieves only within a given file.\"\"\"\n",
        "    filters = MetadataFilters(\n",
        "        filters=[\n",
        "            MetadataFilter(key=\"file_path\", operator=FilterOperator.EQ, value=file),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    def chunk_retriever_fn(query: str) -> str:\n",
        "        retriever = index.as_retriever(similarity_top_k=5, filters=filters)\n",
        "        nodes = retriever.retrieve(query)\n",
        "\n",
        "        full_text = \"\\n\\n========================\\n\\n\".join(\n",
        "            [n.get_content(metadata_mode=\"all\") for n in nodes]\n",
        "        )\n",
        "\n",
        "        return full_text\n",
        "\n",
        "    # define name as a function of the file\n",
        "    fn_name = Path(file).stem + \"_retrieve\"\n",
        "\n",
        "    tool_description = f\"Retrieves a small set of relevant document chunks from {file}.\"\n",
        "    if file_description is not None:\n",
        "        tool_description += f\"\\n\\nFile Description: {file_description}\"\n",
        "\n",
        "    tool = FunctionTool.from_defaults(\n",
        "        fn=chunk_retriever_fn, name=fn_name, description=tool_description\n",
        "    )\n",
        "\n",
        "    return tool\n",
        "\n",
        "\n",
        "# generate tools\n",
        "tools = []\n",
        "for f in files:\n",
        "    tools.append(generate_tool(f, file_description=file_dicts[f][\"summary\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9106c7d-8c43-4443-a192-94d5f3a54ad4",
      "metadata": {
        "id": "f9106c7d-8c43-4443-a192-94d5f3a54ad4"
      },
      "outputs": [],
      "source": [
        "# validate an existing function\n",
        "tools[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "688ebbad-3200-4818-aa48-705eda21503e",
      "metadata": {
        "id": "688ebbad-3200-4818-aa48-705eda21503e"
      },
      "source": [
        "## Build Workflow\n",
        "\n",
        "Let's build a workflow that can iterate through the extracted keys/questions from the RFP, and fill them out!\n",
        "\n",
        "The user specifies an RFP document as input. The workflow then goes through the following steps:\n",
        "1. We parse the RFP template using LlamaParse\n",
        "2. We then extract out the relevant questions we'd want to ask the knowledge base given the instructions in the RFP\n",
        "3. For each question, we query the knowledge base using a specialized agent to generate a response. The agent is equipped with a set of retrieval tools over the data.\n",
        "4. We concatenate the questions/answers into a list of dictionaries.\n",
        "5. Given the question/answer pairs, we feed it along with the source RFP template into a prompt to generate the final report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740535e8-714b-4744-a515-5007834eea6d",
      "metadata": {
        "id": "740535e8-714b-4744-a515-5007834eea6d"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.workflow import (\n",
        "    Event,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Context,\n",
        "    Workflow,\n",
        "    step,\n",
        ")\n",
        "from llama_index.core.llms import LLM\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel\n",
        "from llama_index.core.schema import Document\n",
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "\n",
        "_logger = logging.getLogger(__name__)\n",
        "_logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "# this is the research agent's system prompt, tasked with answering a specific question\n",
        "AGENT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a research agent tasked with filling out a specific form key/question with the appropriate value, given a bank of context.\n",
        "You are given a specific form key/question. Think step-by-step and use the existing set of tools to help answer the question.\n",
        "\n",
        "You MUST always use at least one tool to answer each question. Only after you've determined that existing tools do not \\\n",
        "answer the question should you try to reason from first principles and prior knowledge to answer the question.\n",
        "\n",
        "You MUST try to answer the question instead of only saying 'I dont know'.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# This is the prompt tasked with extracting information from an RFP file.\n",
        "EXTRACT_KEYS_PROMPT = \"\"\"\\\n",
        "You are provided an entire RFP document, or a large subsection from it.\n",
        "\n",
        "We wish to generate a response to the RFP in a way that adheres to the instructions within the RFP, \\\n",
        "including the specific sections that an RFP response should contain, and the content that would need to go \\\n",
        "into each section.\n",
        "\n",
        "Your task is to extract out a list of \"questions\", where each question corresponds to a specific section that is required in the RFP response.\n",
        "Put another way, after we extract out the questions we will go through each question and answer each one \\\n",
        "with our downstream research assistant, and the combined\n",
        "question:answer pairs will constitute the full RFP response.\n",
        "\n",
        "You must TRY to extract out questions that can be answered by the provided knowledge base. We provide the list of file metadata below.\n",
        "\n",
        "Additional requirements:\n",
        "- Try to make the questions SPECIFIC given your knowledge of the RFP and the knowledge base. Instead of asking a question like \\\n",
        "\"How do we ensure security\" ask a question that actually addresses a security requirement in the RFP and can be addressed by the knowledge base.\n",
        "- Make sure the questions are comprehensive and addresses all the RFP requirements.\n",
        "- Make sure each question is descriptive - this gives our downstream assistant context to fill out the value for that question\n",
        "- Extract out all the questions as a list of strings.\n",
        "\n",
        "\n",
        "Knowledge Base Files:\n",
        "{file_metadata}\n",
        "\n",
        "RFP Full Template:\n",
        "{rfp_text}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# this is the prompt that generates the final RFP response given the original template text and question-answer pairs.\n",
        "GENERATE_OUTPUT_PROMPT = \"\"\"\\\n",
        "You are an expert analyst.\n",
        "Your task is to generate an RFP response according to the given RFP and question/answer pairs.\n",
        "\n",
        "You are given the following RFP and qa pairs:\n",
        "\n",
        "<rfp_document>\n",
        "{output_template}\n",
        "</rfp_document>\n",
        "\n",
        "<question_answer_pairs>\n",
        "{answers}\n",
        "</question_answer_pairs>\n",
        "\n",
        "Not every question has an appropriate answer. This is because the agent tasked with answering the question did not have the right context to answer it.\n",
        "If this is the case, you MUST come up with an answer that is reasonable. You CANNOT say that you are unsure in any area of the RFP response.\n",
        "\n",
        "\n",
        "Please generate the output according to the template and the answers, in markdown format.\n",
        "Directly output the generated markdown content, do not add any additional text, such as \"```markdown\" or \"Here is the output:\".\n",
        "Follow the original format of the template as closely as possible, and fill in the answers into the appropriate sections.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class OutputQuestions(BaseModel):\n",
        "    \"\"\"List of keys that make up the sections of the RFP response.\"\"\"\n",
        "\n",
        "    questions: List[str]\n",
        "\n",
        "\n",
        "class OutputTemplateEvent(Event):\n",
        "    docs: List[Document]\n",
        "\n",
        "\n",
        "class QuestionsExtractedEvent(Event):\n",
        "    questions: List[str]\n",
        "\n",
        "\n",
        "class HandleQuestionEvent(Event):\n",
        "    question: str\n",
        "\n",
        "\n",
        "class QuestionAnsweredEvent(Event):\n",
        "    question: str\n",
        "    answer: str\n",
        "\n",
        "\n",
        "class CollectedAnswersEvent(Event):\n",
        "    combined_answers: str\n",
        "\n",
        "\n",
        "class LogEvent(Event):\n",
        "    msg: str\n",
        "    delta: bool = False\n",
        "\n",
        "\n",
        "class RFPWorkflow(Workflow):\n",
        "    \"\"\"RFP workflow.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tools,\n",
        "        parser: LlamaParse,\n",
        "        llm: LLM | None = None,\n",
        "        similarity_top_k: int = 20,\n",
        "        output_dir: str = data_out_dir,\n",
        "        agent_system_prompt: str = AGENT_SYSTEM_PROMPT,\n",
        "        generate_output_prompt: str = GENERATE_OUTPUT_PROMPT,\n",
        "        extract_keys_prompt: str = EXTRACT_KEYS_PROMPT,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        \"\"\"Init params.\"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.tools = tools\n",
        "\n",
        "        self.parser = parser\n",
        "\n",
        "        self.llm = llm or OpenAI(model=\"gpt-4o-mini\")\n",
        "        self.similarity_top_k = similarity_top_k\n",
        "\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        self.agent_system_prompt = agent_system_prompt\n",
        "        self.extract_keys_prompt = extract_keys_prompt\n",
        "\n",
        "        # if not exists, create\n",
        "        out_path = Path(self.output_dir) / \"workflow_output\"\n",
        "        if not out_path.exists():\n",
        "            out_path.mkdir(parents=True, exist_ok=True)\n",
        "            os.chmod(str(out_path), 0o0777)\n",
        "\n",
        "        self.generate_output_prompt = PromptTemplate(generate_output_prompt)\n",
        "\n",
        "    @step\n",
        "    async def parse_output_template(\n",
        "        self, ctx: Context, ev: StartEvent\n",
        "    ) -> OutputTemplateEvent:\n",
        "        # load output template file\n",
        "        out_template_path = Path(\n",
        "            f\"{self.output_dir}/workflow_output/output_template.jsonl\"\n",
        "        )\n",
        "        if out_template_path.exists():\n",
        "            with open(out_template_path, \"r\") as f:\n",
        "                docs = [Document.model_validate_json(line) for line in f]\n",
        "        else:\n",
        "            docs = await self.parser.aload_data(ev.rfp_template_path)\n",
        "            # save output template to file\n",
        "            with open(out_template_path, \"w\") as f:\n",
        "                for doc in docs:\n",
        "                    f.write(doc.model_dump_json())\n",
        "                    f.write(\"\\n\")\n",
        "\n",
        "        await ctx.set(\"output_template\", docs)\n",
        "        return OutputTemplateEvent(docs=docs)\n",
        "\n",
        "    @step\n",
        "    async def extract_questions(\n",
        "        self, ctx: Context, ev: OutputTemplateEvent\n",
        "    ) -> HandleQuestionEvent:\n",
        "        docs = ev.docs\n",
        "\n",
        "        # save all_questions to file\n",
        "        out_keys_path = Path(f\"{self.output_dir}/workflow_output/all_keys.txt\")\n",
        "        if out_keys_path.exists():\n",
        "            with open(out_keys_path, \"r\") as f:\n",
        "                output_qs = [q.strip() for q in f.readlines()]\n",
        "        else:\n",
        "            # try stuffing all text into the prompt\n",
        "            all_text = \"\\n\\n\".join([d.get_content(metadata_mode=\"all\") for d in docs])\n",
        "            prompt = PromptTemplate(template=self.extract_keys_prompt)\n",
        "\n",
        "            file_metadata = \"\\n\\n\".join(\n",
        "                [\n",
        "                    f\"Name:{t.metadata.name}\\nDescription:{t.metadata.description}\"\n",
        "                    for t in tools\n",
        "                ]\n",
        "            )\n",
        "            try:\n",
        "                if self._verbose:\n",
        "                    ctx.write_event_to_stream(\n",
        "                        LogEvent(msg=\">> Extracting questions from LLM\")\n",
        "                    )\n",
        "\n",
        "                output_qs = self.llm.structured_predict(\n",
        "                    OutputQuestions,\n",
        "                    prompt,\n",
        "                    file_metadata=file_metadata,\n",
        "                    rfp_text=all_text,\n",
        "                ).questions\n",
        "\n",
        "                if self._verbose:\n",
        "                    qs_text = \"\\n\".join([f\"* {q}\" for q in output_qs])\n",
        "                    ctx.write_event_to_stream(LogEvent(msg=f\">> Questions:\\n{qs_text}\"))\n",
        "\n",
        "            except Exception as e:\n",
        "                _logger.error(f\"Error extracting questions from page: {all_text}\")\n",
        "                _logger.error(e)\n",
        "\n",
        "            with open(out_keys_path, \"w\") as f:\n",
        "                f.write(\"\\n\".join(output_qs))\n",
        "\n",
        "        await ctx.set(\"num_to_collect\", len(output_qs))\n",
        "\n",
        "        for question in output_qs:\n",
        "            ctx.send_event(HandleQuestionEvent(question=question))\n",
        "\n",
        "        return None\n",
        "\n",
        "    @step\n",
        "    async def handle_question(\n",
        "        self, ctx: Context, ev: HandleQuestionEvent\n",
        "    ) -> QuestionAnsweredEvent:\n",
        "        question = ev.question\n",
        "\n",
        "        # initialize a Function Calling \"research\" agent where given a task, it can pull responses from relevant tools and synthesize over it\n",
        "        research_agent = FunctionCallingAgentWorker.from_tools(\n",
        "            tools, llm=llm, verbose=False, system_prompt=self.agent_system_prompt\n",
        "        ).as_agent()\n",
        "\n",
        "        # ensure the agent's memory is cleared\n",
        "        response = await research_agent.aquery(question)\n",
        "\n",
        "        if self._verbose:\n",
        "            # instead of printing the message directly, write the event to stream!\n",
        "            msg = f\">> Asked question: {question}\\n>> Got response: {str(response)}\"\n",
        "            ctx.write_event_to_stream(LogEvent(msg=msg))\n",
        "\n",
        "        return QuestionAnsweredEvent(question=question, answer=str(response))\n",
        "\n",
        "    @step\n",
        "    async def combine_answers(\n",
        "        self, ctx: Context, ev: QuestionAnsweredEvent\n",
        "    ) -> CollectedAnswersEvent:\n",
        "        num_to_collect = await ctx.get(\"num_to_collect\")\n",
        "        results = ctx.collect_events(ev, [QuestionAnsweredEvent] * num_to_collect)\n",
        "        if results is None:\n",
        "            return None\n",
        "\n",
        "        combined_answers = \"\\n\".join([result.model_dump_json() for result in results])\n",
        "        # save combined_answers to file\n",
        "        with open(\n",
        "            f\"{self.output_dir}/workflow_output/combined_answers.jsonl\", \"w\"\n",
        "        ) as f:\n",
        "            f.write(combined_answers)\n",
        "\n",
        "        return CollectedAnswersEvent(combined_answers=combined_answers)\n",
        "\n",
        "    @step\n",
        "    async def generate_output(\n",
        "        self, ctx: Context, ev: CollectedAnswersEvent\n",
        "    ) -> StopEvent:\n",
        "        output_template = await ctx.get(\"output_template\")\n",
        "        output_template = \"\\n\".join(\n",
        "            [doc.get_content(\"none\") for doc in output_template]\n",
        "        )\n",
        "\n",
        "        if self._verbose:\n",
        "            ctx.write_event_to_stream(LogEvent(msg=\">> GENERATING FINAL OUTPUT\"))\n",
        "\n",
        "        resp = await self.llm.astream(\n",
        "            self.generate_output_prompt,\n",
        "            output_template=output_template,\n",
        "            answers=ev.combined_answers,\n",
        "        )\n",
        "\n",
        "        final_output = \"\"\n",
        "        async for r in resp:\n",
        "            ctx.write_event_to_stream(LogEvent(msg=r, delta=True))\n",
        "            final_output += r\n",
        "\n",
        "        # save final_output to file\n",
        "        with open(f\"{self.output_dir}/workflow_output/final_output.md\", \"w\") as f:\n",
        "            f.write(final_output)\n",
        "\n",
        "        return StopEvent(result=final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b895c3b1-6ea1-41c3-bcf9-e42c2a4beb7e",
      "metadata": {
        "id": "b895c3b1-6ea1-41c3-bcf9-e42c2a4beb7e"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o\")\n",
        "workflow = RFPWorkflow(\n",
        "    tools,\n",
        "    parser=parser,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    timeout=None,  # don't worry about timeout to make sure it completes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6edb19c4-9133-4a3f-b0bd-adf14399e21b",
      "metadata": {
        "id": "6edb19c4-9133-4a3f-b0bd-adf14399e21b"
      },
      "source": [
        "#### Visualize the workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa35ed08-edc1-4bd3-b80d-6e745367128f",
      "metadata": {
        "id": "aa35ed08-edc1-4bd3-b80d-6e745367128f"
      },
      "outputs": [],
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(RFPWorkflow, filename=\"rfp_workflow.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c2effc-350a-46e6-b325-45c1b24b6e89",
      "metadata": {
        "id": "13c2effc-350a-46e6-b325-45c1b24b6e89"
      },
      "source": [
        "## Run the Workflow\n",
        "\n",
        "Let's run the full workflow and generate the output!\n",
        "\n",
        "This will take 5-20 minutes to run and complete. You can inspect the intermediate verbose outputs below as the intermediate questions/answers are generated. The response is streamed back to the user at the end - the response itself is quite long so will take a while to complete! You can also integrate with an observability provider like LlamaTrace/Arize Phoenix in order to view the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c7db55-aff9-4d20-aa13-4e0931b39f66",
      "metadata": {
        "id": "68c7db55-aff9-4d20-aa13-4e0931b39f66"
      },
      "outputs": [],
      "source": [
        "handler = workflow.run(rfp_template_path=str(Path(data_dir) / \"jedi_cloud_rfp.pdf\"))\n",
        "async for event in handler.stream_events():\n",
        "    if isinstance(event, LogEvent):\n",
        "        if event.delta:\n",
        "            print(event.msg, end=\"\")\n",
        "        else:\n",
        "            print(event.msg)\n",
        "\n",
        "response = await handler\n",
        "print(str(response))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama_parse",
      "language": "python",
      "name": "llama_parse"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}